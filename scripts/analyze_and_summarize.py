#!/usr/bin/env python3
"""
Script t√≠ch h·ª£p: Ph√¢n t√≠ch jobs m·ªõi v√† generate summary
Ch·∫°y local sau khi pull data t·ª´ GitHub
"""

import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from ai.analyser import analyse_job
from ai.summarizer import generate_daily_summary, generate_weekly_summary
from utils.logger import setup_logger
import json

# Setup logger
logger = setup_logger('analyze_and_summarize')

def main():
    """Analyze new jobs and generate summaries"""
    print("=" * 60)
    print("ü§ñ AI Analysis & Summary")
    print("=" * 60)
    
    # Load new jobs
    jobs_file = Path(__file__).parent.parent / 'data' / 'raw_jobs.jsonl'
    if not jobs_file.exists():
        print("‚ùå Kh√¥ng t√¨m th·∫•y data/raw_jobs.jsonl")
        return
    
    # Load jobs t·ª´ 24h g·∫ßn ƒë√¢y
    from datetime import datetime, timedelta
    cutoff = datetime.utcnow() - timedelta(hours=24)
    new_jobs = []
    
    with open(jobs_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    job = json.loads(line)
                    crawled_at = job.get('crawled_at', '')
                    if crawled_at:
                        try:
                            job_date = datetime.fromisoformat(crawled_at.replace('Z', '+00:00'))
                            if job_date >= cutoff:
                                new_jobs.append(job)
                        except:
                            pass
                except:
                    pass
    
    print(f"\nüìä T√¨m th·∫•y {len(new_jobs)} jobs m·ªõi trong 24h")
    
    if not new_jobs:
        print("‚ÑπÔ∏è  Kh√¥ng c√≥ job m·ªõi ƒë·ªÉ ph√¢n t√≠ch")
        # Skip AI analysis n·∫øu kh√¥ng c√≥ jobs m·ªõi
        analyzed = []
    elif len(new_jobs) > 50:
        # N·∫øu qu√° nhi·ªÅu jobs m·ªõi, ch·ªâ ph√¢n t√≠ch top 3 ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian
        print(f"\n‚ö†Ô∏è  Qu√° nhi·ªÅu jobs m·ªõi ({len(new_jobs)}), ch·ªâ ph√¢n t√≠ch top 3...")
        analyzed = []
        for i, job in enumerate(new_jobs[:3], 1):
            print(f"[{i}/3] Analyzing: {job.get('title', 'N/A')[:50]}...", end=' ', flush=True)
            try:
                from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
                
                def run_analysis():
                    return analyse_job(job)
                
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(run_analysis)
                    try:
                        analysis = future.result(timeout=60)
                        analyzed.append({
                            'job': job,
                            'analysis': analysis
                        })
                        print("[OK]")
                    except FutureTimeoutError:
                        print("[TIMEOUT]")
                        logger.warning(f"AI analysis timeout for job {job.get('job_id', 'unknown')}")
            except Exception as e:
                print(f"[FAIL] Error: {str(e)[:30]}")
                logger.error(f"Error analyzing job {job.get('job_id', 'unknown')}: {e}")
    else:
        # Analyze top 5 jobs (gi·∫£m t·ª´ 10 xu·ªëng 5 ƒë·ªÉ nhanh h∆°n)
        print(f"\nüîç Ph√¢n t√≠ch top 5 jobs...")
        analyzed = []
        for i, job in enumerate(new_jobs[:5], 1):
            print(f"[{i}/5] Analyzing: {job.get('title', 'N/A')[:50]}...", end=' ', flush=True)
            try:
                # Th√™m timeout cho AI analysis (60 gi√¢y m·ªói job) - d√πng threading cho cross-platform
                from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
                import threading
                
                def run_analysis():
                    return analyse_job(job)
                
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(run_analysis)
                    try:
                        analysis = future.result(timeout=60)  # 60 seconds timeout
                        analyzed.append({
                            'job': job,
                            'analysis': analysis
                        })
                        print("[OK]")
                    except FutureTimeoutError:
                        print("[TIMEOUT]")
                        logger.warning(f"AI analysis timeout for job {job.get('job_id', 'unknown')} after 60s")
                        future.cancel()
            except Exception as e:
                print(f"[FAIL] Error: {str(e)[:30]}")
                logger.error(f"Error analyzing job {job.get('job_id', 'unknown')}: {e}")
        
        # Save analyses
        if analyzed:
            analyses_dir = Path(__file__).parent.parent / 'data' / 'analyses'
            analyses_dir.mkdir(parents=True, exist_ok=True)
            
            analyses_file = analyses_dir / f"analyses_{datetime.utcnow().strftime('%Y%m%d')}.jsonl"
            with open(analyses_file, 'w', encoding='utf-8') as f:
                for item in analyzed:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
            print(f"\n‚úÖ ƒê√£ l∆∞u {len(analyzed)} analyses v√†o {analyses_file.name}")
    
    # Generate daily summary (ch·ªâ n·∫øu c√≥ jobs m·ªõi)
    if new_jobs:
        print(f"\nüìù Generating daily summary...")
        try:
            daily_summary = generate_daily_summary()
            print(f"‚úÖ Daily summary: {daily_summary.get('total_jobs', 0)} jobs")
            print(f"   Top keywords: {', '.join([k['keyword'] for k in daily_summary.get('top_keywords', [])[:5]])}")
        except Exception as e:
            print(f"‚úó Error generating daily summary: {str(e)[:50]}")
    else:
        print(f"\n‚è≠Ô∏è  Skipping daily summary (kh√¥ng c√≥ jobs m·ªõi)")
    
    # Generate weekly summary (ch·ªâ ch·∫°y v√†o Ch·ªß nh·∫≠t)
    from datetime import datetime
    if datetime.utcnow().weekday() == 6:  # Sunday
        print(f"\nüìä Generating weekly summary...")
        try:
            weekly_summary = generate_weekly_summary()
            print(f"‚úÖ Weekly summary: {weekly_summary.get('total_jobs', 0)} jobs")
        except Exception as e:
            print(f"‚úó Error generating weekly summary: {str(e)[:50]}")
    
    print("\n" + "=" * 60)

if __name__ == '__main__':
    main()

