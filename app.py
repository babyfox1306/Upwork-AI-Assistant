#!/usr/bin/env python3
"""
Streamlit Web App - Upwork AI Assistant Chat Interface
"""

import streamlit as st
import yaml
from pathlib import Path
import chromadb
from chromadb.config import Settings
import sys

try:
    from ollama import Client
    OLLAMA_CLIENT = True
except ImportError:
    try:
        import ollama
        OLLAMA_CLIENT = False
    except ImportError:
        st.error("‚ö† L·ªói: Kh√¥ng t√¨m th·∫•y ollama. H√£y c√†i: pip install ollama")
        st.stop()

sys.path.insert(0, str(Path(__file__).parent))

from utils.embedding import get_embedding_model

# Load config
@st.cache_resource
def load_config():
    config_path = Path(__file__).parent / 'config' / 'config.yaml'
    profile_path = Path(__file__).parent / 'config' / 'profile.yaml'
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    with open(profile_path, 'r', encoding='utf-8') as f:
        profile = yaml.safe_load(f)
    
    return config, profile

@st.cache_resource
def init_chromadb():
    """Kh·ªüi t·∫°o ChromaDB - t·ª± ƒë·ªông t·∫°o collection n·∫øu ch∆∞a c√≥"""
    config, _ = load_config()
    chromadb_config = config['chromadb']
    persist_dir = Path(__file__).parent / chromadb_config['persist_directory']
    persist_dir.mkdir(parents=True, exist_ok=True)
    
    client = chromadb.PersistentClient(
        path=str(persist_dir),
        settings=Settings(anonymized_telemetry=False)
    )
    # D√πng get_or_create_collection ƒë·ªÉ t·ª± ƒë·ªông t·∫°o n·∫øu ch∆∞a c√≥
    collection = client.get_or_create_collection(
        name=chromadb_config['collection_name'],
        metadata={"hnsw:space": "cosine"}
    )
    return collection

@st.cache_resource
def _get_cached_embedding_model():
    """Cache SentenceTransformer model ƒë·ªÉ tƒÉng t·ªëc (Streamlit cache)"""
    return get_embedding_model()

@st.cache_data
def _load_ai_rules():
    """Cache AI rules files ƒë·ªÉ tr√°nh ƒë·ªçc file m·ªói l·∫ßn chat"""
    rules_dir = Path(__file__).parent / 'ai_rules'
    rules = {
        'system_instruction': '',
        'rulebook': '',
        'hardware': '',
        'profile_context': ''
    }
    
    analysis_file = rules_dir / 'analysis.md'
    if analysis_file.exists():
        with open(analysis_file, 'r', encoding='utf-8') as f:
            rules['system_instruction'] = f.read()
    
    rules_file = rules_dir / 'upwork_rules.md'
    if rules_file.exists():
        with open(rules_file, 'r', encoding='utf-8') as f:
            rules['rulebook'] = f.read()
    
    hardware_file = rules_dir / 'hardware.md'
    if hardware_file.exists():
        with open(hardware_file, 'r', encoding='utf-8') as f:
            rules['hardware'] = f.read()
    
    profile_context_file = rules_dir / 'profile_context.md'
    if profile_context_file.exists():
        with open(profile_context_file, 'r', encoding='utf-8') as f:
            rules['profile_context'] = f.read()
    
    return rules

def search_jobs(collection, query_text, top_k=10):
    """Search jobs trong ChromaDB"""
    try:
        model = _get_cached_embedding_model()
        query_embedding = model.encode([query_text])[0].tolist()
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        jobs = []
        # Check if results are valid
        if results and 'ids' in results and results['ids'] and len(results['ids']) > 0:
            if len(results['ids'][0]) > 0:
                for i in range(len(results['ids'][0])):
                    job = {
                        'job_id': results['ids'][0][i],
                        'title': results['metadatas'][0][i].get('title', ''),
                        'description': results['documents'][0][i],
                        'budget': results['metadatas'][0][i].get('budget', ''),
                        'proposals': results['metadatas'][0][i].get('proposals', ''),
                        'client_country': results['metadatas'][0][i].get('client_country', ''),
                        'category': results['metadatas'][0][i].get('category', ''),
                        'link': results['metadatas'][0][i].get('link', ''),
                        'source': results['metadatas'][0][i].get('source', 'Unknown'),
                    }
                    jobs.append(job)
        
        return jobs
    except Exception as e:
        st.error(f"L·ªói khi search jobs: {e}")
        st.info("üí° Tip: C√≥ th·ªÉ c·∫ßn reinstall ChromaDB: pip install --upgrade chromadb>=1.3.5")
        return []

def chat_with_ai(user_input, collection, conversation_history):
    """Chat v·ªõi AI"""
    config, profile = load_config()
    ollama_config = config['ollama']
    base_url = ollama_config.get('base_url', 'http://localhost:11434')
    
    # Build context
    context = f"""
Profile Tu·∫•n Anh (freelancer):
- Name: {profile.get('name', 'Tu·∫•n Anh')}
- Title: {profile.get('title', 'Python Developer')}
- Skills: {', '.join(profile.get('skills', []))}
- Experience: {profile.get('experience', 0)} nƒÉm
- Rate: {profile.get('rate', '')}
- Work Style: {profile.get('work_style', 'Demo-first')}
"""
    
    # N·∫øu user h·ªèi v·ªÅ jobs, search tr∆∞·ªõc (ch·ªâ khi c·∫ßn)
    if any(keyword in user_input.lower() for keyword in ['job', 'vi·ªác', 't√¨m', 'search', 'ph√¢n t√≠ch']):
        jobs = search_jobs(collection, user_input, top_k=5)
        if jobs:
            context += f"\n\nJobs t√¨m ƒë∆∞·ª£c:\n"
            for i, job in enumerate(jobs, 1):
                context += f"{i}. {job['title']}\n   Budget: {job.get('budget', 'N/A')}\n   Link: {job.get('link', 'N/A')}\n\n"
    
    # Load AI rules t·ª´ cache (nhanh h∆°n)
    ai_rules = _load_ai_rules()
    system_instruction = ai_rules['system_instruction']
    rulebook = ai_rules['rulebook']
    hardware = ai_rules['hardware']
    profile_context = ai_rules['profile_context']
    
    # Build system prompt ng·∫Øn g·ªçn h∆°n (ch·ªâ load rules khi c·∫ßn)
    # R√∫t g·ªçn personality ƒë·ªÉ prompt ng·∫Øn h∆°n, nhanh h∆°n
    system_prompt = f"""B·∫°n l√† Lysa - AI assistant th√¥ng minh, n√≥i chuy·ªán t·ª± nhi√™n, c√≥ t∆∞ duy logic. H·ªó tr·ª£ Tu·∫•n Anh (freelancer) t√¨m jobs, ph√¢n t√≠ch, vi·∫øt proposal.

Tone: T·ª± nhi√™n nh∆∞ b·∫°n b√®, kh√¥ng formal, th·ª±c t·∫ø, kh√¥ng v√≤ng vo.

{profile_context[:500] if profile_context else ''}"""
    
    # Ch·ªâ th√™m rules khi user h·ªèi v·ªÅ ph√¢n t√≠ch ho·∫∑c proposal
    if any(kw in user_input.lower() for kw in ['ph√¢n t√≠ch', 'analyze', 'proposal', 'vi·∫øt']):
        system_prompt += f"\n\n{system_instruction[:300] if system_instruction else ''}"
        system_prompt += f"\n{rulebook[:300] if rulebook else ''}"
    
    messages = [
        {'role': 'system', 'content': system_prompt + '\n\n' + context}
    ]
    
    # Add conversation history
    messages.extend(conversation_history[-4:])
    
    # Add user input
    messages.append({'role': 'user', 'content': user_input})
    
    try:
        if OLLAMA_CLIENT:
            client = Client(host=base_url, timeout=30.0)  # Timeout ng·∫Øn h∆°n
            response = client.chat(
                model=ollama_config['model'],
                messages=messages,
                options={
                    'temperature': 0.5,  # Gi·∫£m xu·ªëng 0.5 ƒë·ªÉ nhanh h∆°n nh∆∞ng v·∫´n t·ª± nhi√™n
                    'num_predict': 500,  # Gi·∫£m xu·ªëng ƒë·ªÉ nhanh h∆°n
                    'top_p': 0.85,
                    'top_k': 40,  # Th√™m top_k ƒë·ªÉ nhanh h∆°n
                }
            )
            return response['message']['content']
        else:
            response = ollama.chat(
                model=ollama_config['model'],
                messages=messages,
                options={
                    'temperature': 0.5,
                    'num_predict': 500,
                    'top_p': 0.85,
                    'top_k': 40,
                }
            )
            return response['message']['content']
    except Exception as e:
        return f"L·ªói: {e}. ƒê·∫£m b·∫£o Ollama ƒëang ch·∫°y: ollama serve"

# Streamlit UI
st.set_page_config(
    page_title="Upwork AI Assistant",
    page_icon="üí¨",
    layout="wide"
)

st.title("üí¨ Upwork AI Assistant")
st.caption("Chat v·ªõi AI ƒë·ªÉ ph√¢n t√≠ch jobs, t∆∞ v·∫•n, v√† vi·∫øt proposal")

# Sidebar
with st.sidebar:
    st.header("üìä Th√¥ng tin")
    config, profile = load_config()
    st.write(f"**Skills:** {', '.join(profile.get('skills', []))}")
    st.write(f"**Experience:** {profile.get('experience', 0)} nƒÉm")
    st.write(f"**Rate:** {profile.get('rate', '')}")
    
    st.divider()
    
    st.header("üîß Actions")
    if st.button("üîÑ Refresh Jobs"):
        st.cache_resource.clear()
        st.rerun()
    
    if st.button("üì• Sync Data"):
        import subprocess
        with st.spinner("ƒêang sync..."):
            result = subprocess.run(
                ["python", "scripts/local_sync_and_rag.py"],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                st.success("Sync th√†nh c√¥ng!")
            else:
                st.error(f"L·ªói: {result.stderr}")

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = []

# Load ChromaDB
try:
    collection = init_chromadb()
    # Check if collection is empty
    try:
        count = collection.count()
        if count == 0:
            st.warning("‚ö† ChromaDB collection tr·ªëng. H√£y ch·∫°y `update.bat` ƒë·ªÉ sync jobs v√†o database.")
    except:
        pass
except Exception as e:
    st.error(f"L·ªói kh·ªüi t·∫°o ChromaDB: {e}")
    st.info("üí° **Gi·∫£i ph√°p:** Ch·∫°y `update.bat` ƒë·ªÉ t·∫°o collection v√† sync jobs v√†o ChromaDB.")
    st.stop()

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("H·ªèi g√¨ ƒë√≥ v·ªÅ jobs, proposal, ho·∫∑c t∆∞ v·∫•n..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Get AI response
    with st.chat_message("assistant"):
        with st.spinner("ƒêang suy nghƒ©..."):
            response = chat_with_ai(
                prompt, 
                collection, 
                st.session_state.conversation_history
            )
            st.markdown(response)
    
    # Add assistant message
    st.session_state.messages.append({"role": "assistant", "content": response})
    
    # Update conversation history
    st.session_state.conversation_history.append({'role': 'user', 'content': prompt})
    st.session_state.conversation_history.append({'role': 'assistant', 'content': response})

# Quick actions
st.divider()
st.subheader("üí° G·ª£i √Ω c√¢u h·ªèi")

col1, col2, col3 = st.columns(3)

with col1:
    if st.button("üîç T√¨m jobs WordPress"):
        st.session_state.messages.append({"role": "user", "content": "T√¨m jobs WordPress cho anh"})
        st.rerun()

with col2:
    if st.button("üìù Ph√¢n t√≠ch jobs m·ªõi"):
        st.session_state.messages.append({"role": "user", "content": "Ph√¢n t√≠ch jobs m·ªõi nh·∫•t cho Tu·∫•n anh"})
        st.rerun()

with col3:
    if st.button("‚úçÔ∏è Vi·∫øt proposal"):
        st.session_state.messages.append({"role": "user", "content": "H∆∞·ªõng d·∫´n tu·∫•n anh vi·∫øt proposal"})
        st.rerun()

